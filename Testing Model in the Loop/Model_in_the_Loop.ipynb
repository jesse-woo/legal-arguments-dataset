{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06456ffe",
   "metadata": {},
   "source": [
    "# uninstalling and reinstalling transformers because of key error when trying to use the Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44fc73e4-77b2-434d-9049-d7f118dc66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.35.2\n",
      "Uninstalling transformers-4.35.2:\n",
      "  Successfully uninstalled transformers-4.35.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f4d4d2-c8a3-48db-b0f8-f3270bae80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.0.dev0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a946308-e2b3-4519-a99a-ce74756ff610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 18 18:44:07 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.07             Driver Version: 537.34       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:06:00.0  On |                  Off |\n",
      "|  0%   40C    P8              36W / 450W |    924MiB / 24564MiB |     14%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c32377-aa4f-4ace-90f3-8386927b64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HF_AUTH_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a46e367-c8b4-4b7e-8374-efb3404fe757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbd6920-5f21-447a-8ba4-de834ca97c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c7203a-c152-4c7b-9f80-eb1342c14c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "#                                              torch_dtype=\"auto\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\",\n",
    "#                                           torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ed2caa-8948-4fdf-8238-ed7401e96dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and tokenizer locally\n",
    "# model.save_pretrained('./Mistral-7B-OpenOrca')\n",
    "# tokenizer.save_pretrained('./Mistral-7B-OpenOrca')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a83dab",
   "metadata": {},
   "source": [
    "# loading the model locally\n",
    "## - uses around 16 Gb of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e28d7a-0410-4fdc-873a-0e2c3fcc2c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ef5f5c6c9049889916a4d4bb739b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_directory = \"./Mistral-7B-OpenOrca\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_directory,\n",
    "                                             torch_dtype=\"auto\"\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2a6f0",
   "metadata": {},
   "source": [
    "# defining some functions and variables for the model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7be7547-9345-40b2-8176-ea210098bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<|im_start|>system\\n\n",
    "You are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\\n\n",
    "<|im_end|>\\n\n",
    "<|im_start|>user\\n\n",
    "what is the meaning of life?\\n\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da543ed6-274b-4f51-8f82-752c0ccb791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def generate(input_text, system_prompt=\"\",max_length=1024):\n",
    "    if system_prompt != \"\":\n",
    "        system_prompt = f\"\"\"<|im_start|> system\\n{system_prompt}<|im_end|>\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\n",
    "    prompt = f\"\"\"<|im_start|> user\\n{input_text}<|im_end|>\"\"\"\n",
    "    final_prompt = system_prompt + prompt\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    model.to(device)\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=max_length,\n",
    "                             temperature=0.1,\n",
    "                             pad_token_id = 3200,\n",
    "                             do_sample=True)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    # text = text[len(final_prompt):] if text.startswith(final_prompt) else text\n",
    "    text = text.replace(final_prompt, '', 1)\n",
    "    wrapped_text = wrap_text(text)\n",
    "    # print(wrapped_text)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5bba4c4-1c8b-4f04-bd76-8ded3dd1048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSION \n",
      " \n",
      "The issue of student-athlete compensation is com-\n",
      "plex, and it deserves a full, unfettered national discus-\n",
      "sion to ensure the fair treatment of student-athletes \n",
      "while also protecting the matchless beneﬁts that col-\n",
      "lege athletics provide to institutions of higher educa-\n",
      "tion, their students, and the public. The injunction below cuts off that important discussion based on an\n",
      "untenable view of federal antitrust law. This Court\n",
      "should reverse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'extracted_toc_with_conclusion.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "conclusion = df.iloc[0, 3]\n",
    "conclusion_category_column = df.iloc[:, 3]\n",
    "\n",
    "conclusion_category_column_name = 'Conclusion Category'\n",
    "conclusion_category_column.name = conclusion_category_column_name\n",
    "\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f902619",
   "metadata": {},
   "source": [
    "# testing to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09c1f612-47a0-46cf-87f0-848049bd050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 616 ms, sys: 0 ns, total: 616 ms\n",
      "Wall time: 652 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nClassification: Reverse<|im_end|>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "instruction = \"Classify the conclusion into one of those following categories: affirm, deny, reverse, remand, other, or incomplete. Return only the classification. {text}\"\n",
    "instruction = instruction.format(text = conclusion)\n",
    "generate(instruction,\n",
    "         system_prompt=\"You are MistralOrca, a legal expert who specializes in classifying the conclusion types of legal briefs. Write out your short and succinct!\",\n",
    "         max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a8c5f8e2-993c-4398-8e20-2b11e2300d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "81b60814-7159-422e-a7a2-613f1297fc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0626c9",
   "metadata": {},
   "source": [
    "# Classifying the conclusions in the following cells\n",
    "## the \"complex\" conclusion categories are as follows:\n",
    "## -affirm, deny, reverse, remand, part-affirm part-deny, part-affirm part-reverse, part-affirm part-remand, part-deny part-reverse, part-deny part-remand, part-reverse part-remand, other, or incomplete\n",
    "\n",
    "## There is a \"simple\" conclusion category as well, but that will be noted in a different section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "49b913c2-c737-4d7c-91b4-33f2afa82a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSION \n",
      " \n",
      "The issue of student-athlete compensation is com-\n",
      "plex, and it deserves a full, unfettered national discus-\n",
      "sion to ensure the fair treatment of student-athletes \n",
      "while also protecting the matchless beneﬁts that col-\n",
      "lege athletics provide to institutions of higher educa-\n",
      "tion, their students, and the public. The injunction below cuts off that important discussion based on an\n",
      "untenable view of federal antitrust law. This Court\n",
      "should reverse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'extracted_toc_with_conclusion.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "conclusion = df.iloc[0, 3]\n",
    "conclusion_category_column = df.iloc[:, 3].copy()\n",
    "\n",
    "conclusion_category_column_name = 'Complex Conclusion Category'\n",
    "conclusion_category_column.name = conclusion_category_column_name\n",
    "\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7a2f4050-1140-4984-bbb9-f1dd678eff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSION    The issue of student-athlete compensation is complex, and it deserves a full, unfettered national discussion to ensure the fair treatment of student-athletes  while also protecting the matchless beneﬁts that college athletics provide to institutions of higher education, their students, and the public. The injunction below cuts off that important discussion based on an untenable view of federal antitrust law. This Court should reverse.\n"
     ]
    }
   ],
   "source": [
    "formatted_conclusion = first_row.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "\n",
    "print(formatted_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b99e1f5c-1350-4b02-a917-4eac85815ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d14b9f4-4ccc-4f6b-840d-4f897a67f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1362: UserWarning: Input length of input_ids is 1129, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "# for index, conclusion in conclusion_category_column.items():\n",
    "#     # if index > 10:\n",
    "#     #     break\n",
    "#     print(index)\n",
    "#     try:\n",
    "#         conclusion = conclusion.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "#         conclusion = re.sub(' +', ' ', conclusion)\n",
    "    \n",
    "#         # print(conclusion)\n",
    "    \n",
    "#         instruction = \"Classify the conclusion into one of those following categories: affirm, deny, reverse, remand, part-affirm part-deny, part-affirm part-reverse, part-affirm part-remand, part-deny part-reverse, part-deny part-remand, part-reverse part-remand, other, or incomplete. Return only the classification. {text}\"\n",
    "#         instruction = instruction.format(text = conclusion)\n",
    "#         classification = generate(instruction,\n",
    "#              system_prompt=\"You are MistralOrca, a legal expert who specializes in classifying the conclusion types of legal briefs. Write out your short and succinct!\",\n",
    "#              max_length=1024)\n",
    "        \n",
    "#         classification = classification.replace('<|im_end|>', '').strip()\n",
    "#     except:\n",
    "#         print('error')\n",
    "#     conclusion_category_column.at[index] = classification\n",
    "\n",
    "import re\n",
    "\n",
    "for index, conclusion in conclusion_category_column.items():\n",
    "    # if index > 50:\n",
    "    #     break\n",
    "    print(index)\n",
    "\n",
    "    if conclusion is None or conclusion == \"\":\n",
    "        continue\n",
    "\n",
    "    # Start of the loop\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count > 10:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        try:\n",
    "            # for some reason, a couple of conclusions are not strings\n",
    "            if not isinstance(conclusion, str):\n",
    "                conclusion = str(conclusion)\n",
    "            conclusion = conclusion.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "            conclusion = re.sub(' +', ' ', conclusion)\n",
    "\n",
    "            instruction = \"Classify the conclusion into one of those following categories: affirm, deny, reverse, remand, part-affirm part-deny, part-affirm part-reverse, part-affirm part-remand, part-deny part-reverse, part-deny part-remand, part-reverse part-remand, other, or incomplete. Return only the classification. {text}\"\n",
    "            instruction = instruction.format(text=conclusion)\n",
    "            classification = generate(instruction,\n",
    "                system_prompt=\"You are MistralOrca, a legal expert who specializes in classifying the conclusion types of legal briefs. Write out your short and succinct!\",\n",
    "                max_length=1024)\n",
    "\n",
    "            classification = classification.replace('<|im_end|>', '').strip()\n",
    "\n",
    "            # Break the loop if classification is not empty or just spaces\n",
    "            if classification and not classification.isspace():\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print('error:', e)\n",
    "            \n",
    "\n",
    "    # Assign the classification\n",
    "    conclusion_category_column.at[index] = classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "094cc734-7383-42b8-8064-c0295c4d1897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     The classification for this conclusion is: rev...\n",
       "1                       The classification is: reverse.\n",
       "2                        The conclusion type is: affirm\n",
       "3                      The conclusion type is \"affirm\".\n",
       "4                          The conclusion type is: deny\n",
       "5                          The conclusion type is: deny\n",
       "6     The classification for this conclusion is: aff...\n",
       "7                        The conclusion type is: affirm\n",
       "8                        The conclusion type is: affirm\n",
       "9     The classification for this conclusion is: affirm\n",
       "10                       The conclusion type is: affirm\n",
       "Name: Complex Conclusion Category, dtype: object"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion_category_column.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3e05b745-6152-45e6-8cb1-52efd1e710f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_category_column.to_csv('conclusion_classifications_expanded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b215e",
   "metadata": {},
   "source": [
    "# This is where we had the model classify the conclusions into more simplier categories\n",
    "## - affirm, deny, reverse, remand, other, or incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dcbab073-10f6-4b30-8ffe-bf4e80a98aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSION \n",
      " \n",
      "The issue of student-athlete compensation is com-\n",
      "plex, and it deserves a full, unfettered national discus-\n",
      "sion to ensure the fair treatment of student-athletes \n",
      "while also protecting the matchless beneﬁts that col-\n",
      "lege athletics provide to institutions of higher educa-\n",
      "tion, their students, and the public. The injunction below cuts off that important discussion based on an\n",
      "untenable view of federal antitrust law. This Court\n",
      "should reverse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'extracted_toc_with_conclusion.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "conclusion = df.iloc[0, 3]\n",
    "conclusion_category_column = df.iloc[:, 3].copy()\n",
    "\n",
    "conclusion_category_column_name = 'Simple Conclusion Category'\n",
    "conclusion_category_column.name = conclusion_category_column_name\n",
    "\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a97bb9e1-eb34-4d60-968d-623967e17f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCLUSION    The issue of student-athlete compensation is complex, and it deserves a full, unfettered national discussion to ensure the fair treatment of student-athletes  while also protecting the matchless beneﬁts that college athletics provide to institutions of higher education, their students, and the public. The injunction below cuts off that important discussion based on an untenable view of federal antitrust law. This Court should reverse.\n"
     ]
    }
   ],
   "source": [
    "formatted_conclusion = first_row.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "\n",
    "print(formatted_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a9ce7754-4b4f-42a2-925c-c161442f7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1362: UserWarning: Input length of input_ids is 1078, but `max_length` is set to 1024. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for index, conclusion in conclusion_category_column.items():\n",
    "    # if index > 50:\n",
    "    #     break\n",
    "    print(index)\n",
    "\n",
    "    if conclusion is None or conclusion == \"\":\n",
    "        continue\n",
    "\n",
    "    # Start of the loop\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count > 10:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        try:\n",
    "            # for some reason, a couple of conclusions are not strings\n",
    "            if not isinstance(conclusion, str):\n",
    "                conclusion = str(conclusion)\n",
    "            conclusion = conclusion.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "            conclusion = re.sub(' +', ' ', conclusion)\n",
    "\n",
    "            instruction = \"Classify the conclusion into one of the following categories: affirm, deny, reverse, remand, other, or incomplete. Return only the classification. {text}\"\n",
    "            instruction = instruction.format(text=conclusion)\n",
    "            classification = generate(instruction,\n",
    "                system_prompt=\"You are MistralOrca, a legal expert who specializes in classifying the conclusion types of legal briefs. Write out your short and succinct!\",\n",
    "                max_length=1024)\n",
    "\n",
    "            classification = classification.replace('<|im_end|>', '').strip()\n",
    "\n",
    "            # Break the loop if classification is not empty or just spaces\n",
    "            if classification and not classification.isspace():\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print('error:', e)\n",
    "            \n",
    "\n",
    "    # Assign the classification\n",
    "    conclusion_category_column.at[index] = classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e7007412-5202-466c-b9d6-b9fcabd9760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     The classification for this conclusion is: rev...\n",
       "1                               Classification: Reverse\n",
       "2                                Classification: Affirm\n",
       "3                      The conclusion type is \"affirm\".\n",
       "4                        The conclusion type is \"deny\".\n",
       "5                          The conclusion type is: deny\n",
       "6                        The conclusion type is: affirm\n",
       "7                        The conclusion type is: affirm\n",
       "8                                Classification: Affirm\n",
       "9                                Classification: Affirm\n",
       "10                               Classification: Affirm\n",
       "11                              Classification: Reverse\n",
       "12                              Classification: Reverse\n",
       "13                      The conclusion type is: reverse\n",
       "14                               Classification: Affirm\n",
       "15                       The conclusion type is: affirm\n",
       "16                          The classification is: deny\n",
       "17                         The conclusion is to affirm.\n",
       "18    officers or representatives who have the autho...\n",
       "19                              Classification: Reverse\n",
       "20    title=\"Conclusion\"\\n\\nThe Court should grant t...\n",
       "21                              Classification: reverse\n",
       "22                               Classification: Remand\n",
       "23                       The conclusion type is: affirm\n",
       "24                         The conclusion type is: deny\n",
       "25                               Classification: Affirm\n",
       "26                      The conclusion type is: reverse\n",
       "27                       The conclusion type is: other.\n",
       "28                       The conclusion type is: other.\n",
       "29                               Classification: Affirm\n",
       "30                               Classification: Affirm\n",
       "31                               Classification: Affirm\n",
       "32                      The conclusion type is: reverse\n",
       "33                       The conclusion type is: affirm\n",
       "34    Court deny the Petition for a Writ of Mandamus...\n",
       "35    The classification for this conclusion is: rev...\n",
       "36                            The conclusion is \"deny\".\n",
       "37                       The conclusion type is: affirm\n",
       "38                      The conclusion type is: reverse\n",
       "39                               Classification: Affirm\n",
       "40                               Classification: affirm\n",
       "41    Conclusion: The court should consider the evid...\n",
       "42                     The conclusion type is \"affirm\".\n",
       "43                       The conclusion type is: affirm\n",
       "44                       The conclusion type is: affirm\n",
       "45                            The conclusion is \"deny\".\n",
       "46                         The conclusion type is: deny\n",
       "47                       The conclusion type is \"deny\".\n",
       "48    Based on the information provided, the conclus...\n",
       "49                            The conclusion is \"deny\".\n",
       "Name: Simple Conclusion Category, dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion_category_column.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fff30cc3-dc40-47b8-80f8-6b5396fe7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_category_column.to_csv('conclusion_classifications_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "64fe80d8-9b07-4d2d-81ae-8ebd631e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('extracted_toc_with_conclusion.csv')\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df2 = pd.read_csv('conclusion_classifications_simple.csv', header=None)\n",
    "df3 = pd.read_csv('conclusion_classifications_complex.csv', header=None)\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([df1, df2, df3], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('extracted_toc_with_conclusion_classifications.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1fb3b829-bed8-4a19-bcad-833998ccca38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa055c9-8e4c-4b1e-9d52-997a917a806b",
   "metadata": {},
   "source": [
    "# Model Assisted Argument Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6911b51c-a7c0-4a26-a9c7-04fb0dcd72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('extracted_toc_with_conclusion.csv')\n",
    "arguments_column = df1.iloc[:, 1].copy()\n",
    "arguments_column_name = 'Arguments'\n",
    "arguments_column.name = arguments_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b04eab10-0df6-49b7-9d8e-222fa730fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1362: UserWarning: Input length of input_ids is 2627, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for index, toc in arguments_column.items():\n",
    "    # if index > 3:\n",
    "    #     break\n",
    "    print(index)\n",
    "\n",
    "    if toc is None or toc == \"\":\n",
    "        continue\n",
    "\n",
    "    # Start of the loop\n",
    "    count = 0\n",
    "    while True:\n",
    "        if count > 10:\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "        try:\n",
    "            if not isinstance(toc, str):\n",
    "                toc = str(toc)\n",
    "            toc = toc.replace('-\\n', '').replace('\\n', ' ').strip()\n",
    "            toc = re.sub(' +', ' ', toc)\n",
    "\n",
    "            instruction = \"Extract the explicitly stated arguments from the following table of contents. Return only the arguments or 'None' if there are none. {text}\"\n",
    "            instruction = instruction.format(text=toc)\n",
    "            arguments = generate(instruction,\n",
    "                system_prompt=\"You are MistralOrca, a legal expert who specializes in extracting arguments of legal briefs. Write your answer short and succinct!\",\n",
    "                max_length=2048)\n",
    "\n",
    "            arguments = arguments.replace('<|im_end|>', '').strip()\n",
    "\n",
    "            # Break the loop if argument is not empty or just spaces\n",
    "            if arguments and not arguments.isspace():\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print('error:', e)\n",
    "            \n",
    "\n",
    "    # Assign the argument\n",
    "    arguments_column.at[index] = arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0e3e5089-175c-4912-9082-c2d68a6681e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The explicitly stated arguments are:\\n\\n1. The...\n",
       "1    The explicitly stated arguments are:\\n\\n1. The...\n",
       "2    From the provided table of contents, the expli...\n",
       "3    The explicitly stated arguments are:\\n\\n1. The...\n",
       "4    The explicitly stated arguments are:\\n\\n1. Thi...\n",
       "5    From the table of contents, the explicitly sta...\n",
       "6    The explicitly stated arguments are:\\n\\n1. The...\n",
       "7                                                     \n",
       "8    The explicitly stated arguments are:\\n\\n1. The...\n",
       "9    The explicitly stated arguments are:\\n1. The N...\n",
       "Name: Arguments, dtype: object"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_column.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "13aa90a5-eec6-4161-9e00-a65c2b28d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_column.to_csv('extracted_arguments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "97b87b78-8ec2-476e-8e95-95555d580332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('extracted_toc_with_conclusion_classifications.csv')\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df2 = pd.read_csv('extracted_arguments.csv', header=None)\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('extracted_toc_with_classifications_and_arguments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c4473",
   "metadata": {},
   "source": [
    "# Note: \n",
    "## - even after adding all of these to the saved csv, I manually remove a couple of cells to shift the added columns up\n",
    "## - this is because adding in these dataframe added '0' to be the column name with the actual column underneath that\n",
    "## - so the added columns would be one off\n",
    "## - I haven't yet gone into the code to fix that bug; I just manually removed the '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699ceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
