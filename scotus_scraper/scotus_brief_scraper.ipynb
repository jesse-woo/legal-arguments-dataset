{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxuC_XpvWHXP",
    "outputId": "d51c69cd-d0d1-4215-f73c-fd9cd6a70871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (4.12.2)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.18.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jwwoo\\anaconda3\\envs\\selenium_scraper\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.18.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.0 MB 2.3 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/10.0 MB 2.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/10.0 MB 2.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/10.0 MB 2.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.5/10.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/10.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/10.0 MB 2.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.1/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.2/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.5/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.9/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.0/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.2/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.3/10.0 MB 2.4 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.4/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.8/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.0/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.1/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.3/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.4/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.5/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.8/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.0/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.3/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.3/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.6/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.8/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.9/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.0/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.3/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.7/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.7/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.8/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.9/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.1/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.2/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.5/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.7/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.8/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.0/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.4/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.5/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.7/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.9/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.9/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.3/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.3/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.4/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.0/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.5/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 112.6/467.2 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 204.8/467.2 kB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 317.4/467.2 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 399.4/467.2 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 467.2/467.2 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sortedcontainers, python-dotenv, h11, attrs, wsproto, webdriver-manager, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-23.2.0 h11-0.14.0 outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.18.1 sortedcontainers-2.4.0 trio-0.25.0 trio-websocket-0.11.1 webdriver-manager-4.0.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwEssx7WNwn-"
   },
   "source": [
    "Docket numbers retrieved manually from the list maintained by findlaw.com. Only cases decided in 2018 and onward have their documents hosted on the Scotus website. Briefs from 2012 term onward are stored on scotusblog.com, although some of them are in a very messy format (i.e. a picture of a scanned document). Prior briefs used to be hosted by the ABA but they took them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lw-88iQWBRha"
   },
   "outputs": [],
   "source": [
    "docket_nos_path = \"./docket_nos_valid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axmLJQzeBjES",
    "outputId": "03e4e86d-2d14-4080-a530-8787c0c63cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  docket_number  year url_list\n",
      "0         17-71  2018      NaN\n",
      "1       17-1676  2018      NaN\n",
      "2       18-5181  2018      NaN\n",
      "3        17-587  2018      NaN\n",
      "4       17-7894  2018      NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame(columns=['docket_number', 'year', 'url_list'])\n",
    "\n",
    "current_year = None\n",
    "rows = [] \n",
    "\n",
    "with open(docket_nos_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check for year-only lines. The year indicates when the decision was released.\n",
    "        year_match = re.match(r'^<year>\\s+(\\d{4})$', line)\n",
    "        if year_match:\n",
    "            current_year = year_match.group(1)\n",
    "            continue\n",
    "\n",
    "        # Extract the docket number from regular lines\n",
    "        match = re.search(r'No\\.\\s+([\\w-]+)', line)\n",
    "        if match:\n",
    "            docket_number = match.group(1)\n",
    "            # Use the current year for this entry and add to the rows list\n",
    "            rows.append({'docket_number': docket_number, 'year': current_year})\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(rows)], ignore_index=True)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "kEqngQWVFFsZ",
    "outputId": "abc2278b-96be-42ea-c34a-6f4605e35941"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "\n",
    "# Setup WebDriver with headless option\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to process a single docket page\n",
    "def process_docket_page(docket_number):\n",
    "    url = f\"https://www.supremecourt.gov//docket/docketfiles/html/public/{docket_number}.html\"\n",
    "    pdf_urls = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page {url}: {str(e)}\")\n",
    "        return pdf_urls\n",
    "\n",
    "    try:\n",
    "        # Find all table rows\n",
    "        rows = driver.find_elements(By.TAG_NAME, 'tr')\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding table rows: {str(e)}\")\n",
    "        return pdf_urls\n",
    "\n",
    "    for row in rows:\n",
    "        try:\n",
    "            # Check if the row contains the text indicating a brief or document type\n",
    "            # Might need to modify this to exclude cases where Motion is mentioned\n",
    "            # Modify this to exclude cases where \"Supplemental letter\" is in text\n",
    "            pattern = re.compile(r'^(?!.*\\b(motion|supplemental letter)\\b).*\\b(brief of|reply of|brief amicus curiae)\\b', re.IGNORECASE)\n",
    "            \n",
    "            if pattern.search(row.text):\n",
    "                # Find all 'a' elements in this row\n",
    "                # print(f\"Match found in row: {row.text}\")\n",
    "                # Find the next sibling 'td' that contains the link\n",
    "                next_td = row.find_element(By.XPATH, \"./following-sibling::tr[1]/td[2]\")\n",
    "                # print(f\"The next_td is {next_td.get_attribute('outerHTML')}\")\n",
    "                links = next_td.find_elements(By.TAG_NAME, 'a')\n",
    "                # print(f\"The links are: {links}\")\n",
    "                if len(links) > 0:\n",
    "                    for link in links:\n",
    "                        # Check if the link text is \"Main Document\"\n",
    "                        # print(f\"the link is {link.text}\")\n",
    "                        if link.text.strip().lower() == 'main document':\n",
    "                            pdf_urls.append(link.get_attribute('href'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pdf_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# List of docket numbers to process\n",
    "docket_numbers = df['docket_number'].tolist()\n",
    "\n",
    "# Dictionary to hold docket numbers and their associated PDF URLs\n",
    "docket_pdf_urls = {}\n",
    "\n",
    "# Try to do this directly with the dataframe rows, otherwise use a list.\n",
    "for idx, row in df.iterrows():\n",
    "    docket_number = row['docket_number']\n",
    "    pdf_urls = process_docket_page(docket_number)\n",
    "    docket_pdf_urls[docket_number] = pdf_urls\n",
    "    df.at[idx, 'url_list'] = pdf_urls\n",
    "    \n",
    "    time.sleep(random.uniform(1, 2)) # Hopefully avoid rate limits\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "print(len(docket_pdf_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ensure you have the correct headers for the request if needed\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',\n",
    "}\n",
    "\n",
    "num_briefs = 0\n",
    "\n",
    "for docket_number, urls_list in docket_pdf_urls.items():\n",
    "    num = 1\n",
    "    for url in urls_list:\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(f'./brief_pdfs/Docket{docket_number}_Brief{num:03}.pdf', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            num += 1\n",
    "            num_briefs += 1\n",
    "            # print(\"pdf saved successfully, I sleep now\")\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "        else:\n",
    "            print(f\"Failed to download PDF for docket number {docket_number}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You scraped 4377 briefs.\n"
     ]
    }
   ],
   "source": [
    "print(f\"You scraped {num_briefs} briefs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./scraped_briefs.json\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_json('./scraped_briefs.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKOfuQn7ask0"
   },
   "source": [
    "This is for DOJ's archive of OSG briefs, which would serve as a backup. Probably better download directly from supremecourt.gov though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy_okEvTaNIC"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Setup Chrome options for headless mode\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Function to download PDFs for a given type\n",
    "def download_pdfs_for_type(type_value):\n",
    "    driver.get('https://www.justice.gov/osg/supreme-court-briefs')\n",
    "\n",
    "    # Select the type from the dropdown\n",
    "    select = Select(driver.find_element_by_id('edit-type'))\n",
    "    select.select_by_value(type_value)\n",
    "\n",
    "    # Click the search button\n",
    "    driver.find_element_by_id('edit-submit-brief-views').click()\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(5)  # Adjust this delay as necessary\n",
    "\n",
    "    # Use BeautifulSoup to parse driver.page_source, find PDF links, and download them\n",
    "    # Navigate pages if necessary and repeat the download process\n",
    "\n",
    "    # Make sure to handle exceptions and edge cases\n",
    "\n",
    "# List of types to process\n",
    "types = ['merits_stage_brief', 'merits_stage_reply_brief', 'merits_stage_amicus_brief']\n",
    "\n",
    "for type_value in types:\n",
    "    download_pdfs_for_type(type_value)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh3Hg-QTWN1P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
